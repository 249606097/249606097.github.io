<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/L-128x128.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/L-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/L-16x16.png">
  <link rel="mask-icon" href="/images/L-128x128.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://249606097.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: './public/search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="这是软件工程课程第二次学习笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础-2">
<meta property="og:url" content="https://249606097.github.io/2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2/index.html">
<meta property="og:site_name" content="Leo Blog">
<meta property="og:description" content="这是软件工程课程第二次学习笔记。">
<meta property="og:image" content="https://i.loli.net/2020/10/24/PrShXBFlo1DqHCm.png">
<meta property="og:image" content="https://i.loli.net/2020/10/24/VSAuhxPpisaE1nt.png">
<meta property="og:image" content="https://i.loli.net/2020/10/24/4WOX1gJTDdR7GEf.png">
<meta property="og:image" content="https://i.loli.net/2020/10/24/y4orIwc5X3H9gb2.png">
<meta property="og:image" content="https://i.loli.net/2020/10/24/ZVsfNMm7Fhxb1AG.png">
<meta property="article:published_time" content="2020-10-19T04:25:48.000Z">
<meta property="article:modified_time" content="2020-10-25T06:11:09.419Z">
<meta property="article:author" content="Leo">
<meta property="article:tag" content="Python">
<meta property="article:tag" content=" Java">
<meta property="article:tag" content=" Computer Science and Technology">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/10/24/PrShXBFlo1DqHCm.png">

<link rel="canonical" href="https://249606097.github.io/2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>机器学习基础-2 | Leo Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Leo Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Leo Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Awesome!</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://249606097.github.io/2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Leo">
      <meta itemprop="description" content="I just want to do what I want.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Leo Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习基础-2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-19 12:25:48" itemprop="dateCreated datePublished" datetime="2020-10-19T12:25:48+08:00">2020-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-25 14:11:09" itemprop="dateModified" datetime="2020-10-25T14:11:09+08:00">2020-10-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>这是软件工程课程第二次学习笔记。</p>
<a id="more"></a>
<h2 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h2><h3 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h3><h4 id="矩阵的特征值和特征向量"><a href="#矩阵的特征值和特征向量" class="headerlink" title="矩阵的特征值和特征向量"></a>矩阵的特征值和特征向量</h4><p>对于给定的矩阵 $A$，假设其特征值为 $\lambda$，特征向量为 $\boldsymbol x$，则它们之间的关系如下：</p>
<script type="math/tex; mode=display">
A \boldsymbol x = \lambda \boldsymbol x</script><p>从线性变换的角度，矩阵相乘对原始向量同时施加方向变化和尺度变化，而对于有些特殊向量，矩阵的作用只有尺度变化而没有方向变化，这类特殊的向量就是特征向量，尺度变化系数就是特征值。</p>
<h4 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h4><p>矩阵 $A$ 的列秩是 $A$ 的线性独立的纵列的极大数，通常表示为 $r(A)$ 或 $rank(A)$。</p>
<ul>
<li><p>线性方程组的角度：度量矩阵行列之间的相关性。</p>
<p>如果矩阵的各行或各列是线性无关的，矩阵就是满秩的，也就是秩等于行数。</p>
</li>
<li><p>数据分布的角度：表示数据需要的最小的基的数量。</p>
<p>数据分布模式越容易被捕获，即需要的基越少，秩就越小。</p>
<p>数据冗余度越大，需要的基就越少，秩就越小。</p>
<p>若矩阵表示的是结构化信息，如图像、用户-物品等，各行之间存在一定相关性，一般是低秩的。</p>
</li>
</ul>
<h4 id="矩阵的奇异值"><a href="#矩阵的奇异值" class="headerlink" title="矩阵的奇异值"></a>矩阵的奇异值</h4><p>矩阵的奇异值分解 (singular value decomposition, SVD) 是一个能是英语任意矩阵的一种分解方法。</p>
<p>对于一个 $m*n$ 的矩阵 $A$，可分解为</p>
<script type="math/tex; mode=display">
A = U \Sigma V^T</script><p>其中 $U$ 和 $V$ 都是正交矩阵，即有 $UU^T = I$ 和 $VV^T = I$，左奇异矩阵 $U$ 是 $m<em>m$ 的矩阵，右奇异矩阵 $V$ 是 $n</em>n$ 的矩阵， $\Sigma$ 是 $m*n$ 的矩阵，矩阵 $\Sigma$ 除对角线元素以外均为 0，对角线上的元素被称为奇异值。</p>
<p>矩阵 $\Sigma$ 形如</p>
<script type="math/tex; mode=display">
\Sigma = 
\left [
\begin{matrix}
\sigma_1 & 0 & 0 & \cdots & 0 \\
0 & \sigma_2 & 0 & \cdots & 0 \\
0 & 0 & \sigma_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & 0 \\
0 & 0 & 0 & \vdots & 0
\end{matrix}
\right ]_{m \times n}</script><p>较大的奇异值包含了矩阵的主要信息，只保留前 $k$ 个（一般 $k$ 取 $\frac{n}{10}$ 就可以保留足够的信息）较大奇异值及其对应的特征向量可实现数据从 $m<em>n$ 降维到 $(m </em> k + k <em> r + k </em> d)$。</p>
<h4 id="矩阵的低秩近似"><a href="#矩阵的低秩近似" class="headerlink" title="矩阵的低秩近似"></a>矩阵的低秩近似</h4><script type="math/tex; mode=display">
A_{m \times n} = U_{m \times n} \Sigma_{n \times n} V_{n \times n}^T \ \ \ \ \ rank(A) = r \ \ \ \ \ \ \\
\hat{A}_{m \times k} = U_{m \times k} \Sigma_{k \times k} V_{k \times n}^T \ \ \ \ \ \ \ rank(\hat{A}) = k < r</script><p>低秩近似的意义：保留决定数据分布的最重要的模式/方向（丢弃的可能是噪声或其他不关键的信息）</p>
<h3 id="深度学习基础-1"><a href="#深度学习基础-1" class="headerlink" title="深度学习基础"></a>深度学习基础</h3><h4 id="机器学习三要素"><a href="#机器学习三要素" class="headerlink" title="机器学习三要素"></a>机器学习三要素</h4><ul>
<li><p>模型</p>
<p>对要学习问题映射的假设（问题建模，确定假设空间）</p>
</li>
<li><p>策略</p>
<p>从假设空间中学习/选择最优模型的准则（确定目标函数）</p>
</li>
<li><p>算法</p>
<p>根据目标函数求解最优模型的具体计算方法（求解模型参数）</p>
</li>
</ul>
<h4 id="策略设计：训练误差-泛化误差"><a href="#策略设计：训练误差-泛化误差" class="headerlink" title="策略设计：训练误差 -> 泛化误差"></a>策略设计：训练误差 -> 泛化误差</h4><p>“最合适”的模型：机器学习从有限的观测数据中学习出规律，并将总结的规律推广到未观测的样本上（追求泛化性能）</p>
<ul>
<li><p>泛化误差（期望风险）</p>
<script type="math/tex; mode=display">
R(f) = \mathbb{E}_{(x,y) \sim p(x,y)}L(f(x),y)</script><p>其中 $L( \cdot )$ 为损失函数，$(x,y) \sim p(x,y)$ 符合真实数据分布</p>
</li>
<li><p>训练误差（经验风险）</p>
<script type="math/tex; mode=display">
R_{训练}(f) = \frac{1}{N} \sum_{i=1}^{N} L(f(x^{(i)}),y^{(i)})</script><p>模型在训练样本上的平均损失</p>
</li>
<li><p>泛化错误</p>
<script type="math/tex; mode=display">
G = R(f) - R_{训练}(f)</script></li>
</ul>
<p>策略目标：训练误差小 \&amp; 泛化错误低</p>
<p>机器学习的目的是获得小的泛化误差</p>
<ul>
<li>训练误差要小</li>
<li>训练误差与泛化误差足够接近 (generalization gap 要小)</li>
</ul>
<p>PAC 给出了实际训练学习器的目标：从合理数据的训练数据中通过合理计算量学习到了可靠的知识</p>
<ul>
<li>合理数据训练数据：数据集大小</li>
<li>合理计算量：学习训练的时间</li>
<li>可靠的知识：概率的置信度，近似的误差上界</li>
</ul>
<h4 id="策略设计：无免费午餐定理"><a href="#策略设计：无免费午餐定理" class="headerlink" title="策略设计：无免费午餐定理"></a>策略设计：无免费午餐定理</h4><blockquote>
<p>当考虑在所有问题上的平均性能时，任意两个模型都是相同的。</p>
</blockquote>
<p>脱离具体问题，谈“什么学习算法更好”毫无意义。（周志华）</p>
<p>没有任何一个模型可以在所有的学习任务理表现最好。</p>
<p><img src="https://i.loli.net/2020/10/24/PrShXBFlo1DqHCm.png" alt=""></p>
<h4 id="策略设计：奥卡姆剃刀原理"><a href="#策略设计：奥卡姆剃刀原理" class="headerlink" title="策略设计：奥卡姆剃刀原理"></a>策略设计：奥卡姆剃刀原理</h4><blockquote>
<p>如无必要，勿增实体。</p>
</blockquote>
<p>如果多种模型能够同等程度地符合一个问题的观测结果，应该选择其中使用假设最少的。（最简单的模型）</p>
<ul>
<li><p>惩罚大模型复杂度</p>
<p>最小结构风险：</p>
<script type="math/tex; mode=display">
R_{结构}(f) = \frac{1}{N}\sum_{i=1}^{N}L(f(x^{(i)}),y^{(i)}) + \lambda J(f)</script><p>其中 $\lambda J(f)$ 表示模型复杂度，如参数向量的 1 范数。</p>
<p>最大后验概率：</p>
<script type="math/tex; mode=display">
P(h|D) \propto P(D|h) \cdot P(h)</script><p>其中 $P(D|h)$ 是似然，$P(h)$ 是模型的先验，复杂模型是小概率事件。</p>
</li>
<li><p>欠拟合 vs 过拟合</p>
<p>欠拟合：训练集的一般性质尚未被学习器学好。（训练误差大）</p>
<p>提高模型复杂度</p>
<ul>
<li>决策树：拓展分支</li>
<li>神经网络：增加节点个数，增加训练轮数</li>
</ul>
</li>
</ul>
<p>  过拟合：学习器把训练集特点当作样本的一般特点。（训练误差小，测试误差大）</p>
<p>  降低模型复杂度</p>
<ul>
<li>优化目标加正则项</li>
<li>决策树：剪枝</li>
<li>神经网络：early stop, dropout</li>
<li>数据增广（训练集越大，越不容易过拟合）</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>BP 神经网络和损失函数</p>
<p>如果设定 $S(x)=\frac{1}{1+exp(-x)}$，那么损失函数</p>
<script type="math/tex; mode=display">
L = \frac{1}{2} \sum_{k=1}^{N}D(f(x_k),h(x_k)) 
=  \frac{1}{2}\sum_{k=1}^{N}\sum_{i=1}^{c} (f_i(x_k)-x_{k_i}^{(\mathfrak{H})})^2
=\sum_{k=1}^{N}E_k</script><p>令 $E_k = \sum_{i=1}^{c}(f_i(x_k)-x_{k_i}^{(\mathfrak{H})})^2$ 为每个样本的平方损失代价，$(x)_i^{(\mathfrak{H})}=S\left( (W_i^{(\mathfrak{H}-1)})^T(x)^{(\mathfrak{H}-1)} \right)$。</p>
<p>待优化参数：权重 $w_{ij}^{(t)}$</p>
<p>优化算法：梯度下降，需要计算代价函数 L 和梯度 $\frac{\partial L}{\partial w_{ij}^{(t)}} = \sum_{k=1}^{N} \frac{\partial E_k}{\partial w_{ij}^{(t)}}$，更新公式 $w_{ij}^{(t)} \leftarrow w_{ij}^{(t)} - \eta \frac{\partial E_k}{\partial w_{ij}^{(t)}}$。</p>
<ul>
<li><p>平方损失</p>
<p>平方损失是计算在最后一层（输出层）的，使用 Sigmoid 函数</p>
</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w_i} = 2(a - y) \cdot a(1-a) \cdot x_i</script><ul>
<li><p>交叉熵（对数损失函数）</p>
<script type="math/tex; mode=display">
L(a,y)=-y\log a - (1-y)\log (1-a)</script><script type="math/tex; mode=display">
L(y,f(x,\theta))=-\sum_{i=1}^{C}y_i\log f_i(x,\theta)</script><p>对于参数 $w$ 是凸的。平方损失假设高斯分布，而分类问题是二项/多项分布，对数损失与二项/多项分布下的最大似然等价。</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w} = (a-y)\sigma'(z)x = a\sigma'(z) \\
\frac{\partial L}{\partial w_j} = \frac{1}{n}\sum_{x}(\sigma(z)-y)</script><p>输出层的梯度大小不受激活函数的导数影响，损失函数下降速度快。</p>
</li>
</ul>
<h4 id="频率学派-vs-贝叶斯学派"><a href="#频率学派-vs-贝叶斯学派" class="headerlink" title="频率学派 vs 贝叶斯学派"></a>频率学派 vs 贝叶斯学派</h4><p>概率论代表了一种看待世界的方式，对随机事件发生的可能性进行管泛化数学描述，对可能性的不同解读促生了概率轮的两个学派，频率学派和贝叶斯学派。</p>
<p>频率学派</p>
<ul>
<li><p>关注可独立重复的随机试验中单个事件发生的频率</p>
</li>
<li><p>可能性：事件发生频率的极限值，也就是说重复试验次数趋近于无穷大时，事件发生的频率会收敛到真实的概率</p>
</li>
</ul>
<p>假设概率是客观存在且固定的，模型参数是唯一的，需要从有限的观测数据中估计参数。</p>
<p>频率学派对应统计机器学习（线性回归、决策树、支持向量机）</p>
<p>贝叶斯学派</p>
<ul>
<li>关注随机事件的“可信程度”，如天气预报明天下雨的概率（无法重复）</li>
<li>可行性 = 假设 + 数据：数据的作用是对初始假设做出修正，使观察者对概率的主观认识（先验）更接近客观实际（观测）</li>
</ul>
<p>模型参数本身是随机变量，需要估计参数的整个概率分布。</p>
<p>贝叶斯学派对应概率图模型（隐马尔可夫链、条件随机场、主题模型）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>频率学派</th>
<th>贝叶斯学派</th>
</tr>
</thead>
<tbody>
<tr>
<td>利用的信息</td>
<td>观测数据 $X$</td>
<td>观测数据 $X$ + 先验 $p(\theta)$</td>
</tr>
<tr>
<td>参数估计</td>
<td>确定参数值 $\hat{\theta}$</td>
<td>参数后验分布 $p(\theta \mid X)$</td>
</tr>
</tbody>
</table>
</div>
<p>机器学习问题中样本数据有限，核心任务之一就是对模型的参数进行估计。</p>
<p>极大似然估计</p>
<p>使训练数据出现的概率最大化，依次确定模型的参数</p>
<script type="math/tex; mode=display">
\hat{\theta} = argmax_{\theta}P(X|\theta)</script><p>最大后验估计（贝叶斯估计）</p>
<p>根据训练数据和已知先验，估计参数分布；选择参数分布概率最大的值作为估计值</p>
<script type="math/tex; mode=display">
\hat{\theta} = argmax_{\theta}P(\theta |X) = argmax_{\theta}P(X|\theta)P(\theta)</script><h4 id="统计机器学习"><a href="#统计机器学习" class="headerlink" title="统计机器学习"></a>统计机器学习</h4><p>统计机器学习是寻找相关性</p>
<p>Yule-Simpson 悖论表明相关性不可靠，相关关系可能由于 context 等混杂因素而改变。</p>
<blockquote>
<p>当人们尝试探究两种变量（比如新生录取率与性别）是否具有相关性的时候，会分别对之进行分组研究。然而，在分组比较中都占优势的一方，在总评中有时反而是失势的一方。</p>
</blockquote>
<h4 id="相关性和因果性"><a href="#相关性和因果性" class="headerlink" title="相关性和因果性"></a>相关性和因果性</h4><p>相关性：Y 的发生往往伴随着 X。</p>
<p><img src="https://i.loli.net/2020/10/24/VSAuhxPpisaE1nt.png" alt=""></p>
<p>因果性：排除其他因素后，Y 仅由 X 影响。</p>
<p><img src="https://i.loli.net/2020/10/24/4WOX1gJTDdR7GEf.png" alt=""></p>
<p>例如，打伞、地上湿、下雨之间的关系。</p>
<p><img src="https://i.loli.net/2020/10/24/y4orIwc5X3H9gb2.png" alt=""></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络的基本应用：分类、检索、检测、分割。</p>
<p>深度学习的三步</p>
<ol>
<li><p>搭建神经网络的结构</p>
</li>
<li><p>找到一个合适的损失函数</p>
<p>交叉熵损失 (cross entropy loss)，均方误差 (MSE)，等等。</p>
</li>
<li><p>找到一个合适的优化函数，更新参数</p>
<p>反向传播 (BP)，随机梯度下降 (SGD)，等等。</p>
</li>
</ol>
<h3 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h3><ul>
<li><p>常用分类损失</p>
<p>交叉熵损失: $Loss = -\sum y_i\ln y_i^p$</p>
<p>hinge loss: $L(y,f(x))=max(0,1-yf(x))$</p>
</li>
<li><p>常用回归损失</p>
<p>均方误差: $MSE=\sum_{i=1}^{n}(y_i-y_i^p)^2$</p>
<p>平均绝对值误差（L1 损失）: $MAE=\sum_{i=1}^{n}|y_i-y_i^p|$</p>
</li>
</ul>
<h3 id="基本组成结构"><a href="#基本组成结构" class="headerlink" title="基本组成结构"></a>基本组成结构</h3><p>卷积 Convolutional Layer 是对两个实变函数（以实数为自变量的函数）的一种数学操作。</p>
<h4 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h4><p>一维卷积经常用在信号处理中，用于计算信号的延迟累积。</p>
<p>假设一个信号发生器在时刻 $t$ 发出一个信号 $x_t$，其信息的衰减率为 $f_k$，即在 $k-1$ 个时间步长后，信息衰减为原来的 $f_k$ 倍。设 $f_1=1,f_2=\frac{1}{2},f_3=\frac{1}{4}$，在时刻 $t$ 收到的信号 $y_t$ 为当前时刻产生的信息和以前时刻延迟信息的叠加</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_t &= 1 \times x_t + \frac{1}{2} \times x_{t-1} + \frac{1}{4} \times x_{t-2} \\
    &= f_1 \times x_t + f_2 \times x_{t-1} + f_3 \times x_{t-2} \\
    &= \sum_{k=1}^{3}f_k \cdot x_{t-k+1}
\end{aligned}</script><p>此处的 $f = [f_1, f_2, f_3]$ 被称为滤波器 (filter) 或卷积核 (convolutional kernel)。</p>
<p>设滤波器 $f$ 长度为 $m$，它和一个信号序列 $x=[x_1,x_2,x_3,\dots]$ 的卷积记为</p>
<script type="math/tex; mode=display">
y_t = \sum_{k=1}^{m} f_k \cdot x_{t-k+1}</script><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul>
<li>input: 输入</li>
<li>filter/kernel: 过滤器/ 卷积核</li>
<li>weights: 权重</li>
<li>receptive field: 感受野</li>
<li>activation map/feature map: 特征图</li>
<li>stride: 步长</li>
<li>padding: 填补</li>
<li>depth/channel: 深度</li>
<li>output: 输出</li>
</ul>
<p>未加 padding 时输出的特征图大小：$\frac{N-F}{stride} + 1$，加 padding 时输出的特征图的大小：$\frac{N + padding \times 2 - F}{stride} + 1$。</p>
<p>池化 Pooling Layer</p>
<ul>
<li><p>Pooling 保留主要特征的同时减少参数和计算量，防止过拟合，提高模型泛化能力。它一般处于卷积层与卷积层之间，全联接层与全连接层之间</p>
</li>
<li><p>Pooling 的类型：Max pooling 最大值池化（分类问题常用），Average pooling 平均池化</p>
</li>
</ul>
<p>全连接 Fully Connected Layer, FC Layer</p>
<ul>
<li>全联接层的两层之间所有神经元都有权重链接</li>
<li>通常全连接层在卷积神经网络尾部</li>
<li>全连接层参数量通常最大</li>
</ul>
<h4 id="卷积神经网络经典结构"><a href="#卷积神经网络经典结构" class="headerlink" title="卷积神经网络经典结构"></a>卷积神经网络经典结构</h4><p>经典结构有AlexNet, ZFNet, VGG, GoogleNet, ResNet 等。</p>
<p><img src="https://i.loli.net/2020/10/24/ZVsfNMm7Fhxb1AG.png" alt=""></p>
<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>AlexNet 之所以能成功，深度学习之所以能重回历史舞台，原因在于：</p>
<ul>
<li>大数据训练：百万级 ImageNet 图像数据</li>
<li>非线性激活函数：ReLU</li>
<li>放回过拟合：Dropout, Data Augmentation</li>
<li>其他：双 GPU 实现</li>
</ul>
<p>使用 ReLU 函数对比使用 Sigmoid 函数的优点</p>
<ul>
<li>解决了梯度消失问题（在正区间）</li>
<li>计算速度特别快，只需要判断输入是否大于 0</li>
<li>收敛速度快于 Sigmoid 函数</li>
</ul>
<p>应用 Dropout（随机失活），训练时随机关闭部分神经元，测试时整合所有神经元。</p>
<p>数据增强 (data augmentation)</p>
<ul>
<li><p>平移、翻转、对称</p>
<ul>
<li>随机 crop。训练时，对于 256*256 的图片进行随机 crop 到 224*224。</li>
<li>水平翻转，相当于将样本倍增。</li>
</ul>
</li>
<li><p>改变 RGB 通道强度</p>
<p>对 RGB 空间做一个高斯扰动。</p>
<p>每个 RGB 图片的像素 $I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]^T$</p>
<p>$I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]^T + [p_1,p_2,p_3][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]^T$</p>
</li>
</ul>
<h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p>VGG 是一个更深的网络。</p>
<h4 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h4><p>网络总体结构：</p>
<ul>
<li>网络包含 22 个带参数的层（如果考虑 pooling 层就有 27 层），独立成块的层总共约有 100 个</li>
<li>参数量大概是 AlexNet 的 1/12</li>
<li>没有 FC 层</li>
</ul>
<p>Naive Inception 初衷：多卷积核增加特征多样性</p>
<p>Inception V3 进一步对 v2 的参数进行降低。（使用两个 3*3 的卷积核代替一个 5*5 的卷积核），好处是降低了参数量，增加非线性激活函数，使网络产生更多独立特 (disentagled feature)，表征能力更强，训练更快。</p>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>残差学习网络 (deep residual learning network)，深度有 152 层。</p>
<p>残差的思想：去掉相同的主体部分，从而突出微小的变化，可以被用来训练非常深的网络。</p>
<p>对于一个神经网络训练的目的是拟合 $H(x)$，但当网络层数增多时，直接拟合 $H(x)$ 十分困难，所以 ResNet 不直接拟合 $H(x)$，而是拟合 $H(x) - x$，这就是残差 (residual)。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Leo
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://249606097.github.io/2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-2/" title="机器学习基础-2">https://249606097.github.io/2020/10/19/机器学习基础-2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <div>
        
          <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------------- 本文结束 <i class="fa fa-paw"></i> 感谢您的阅读 -------------</div>
    
</div>
        
      </div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" rel="prev" title="机器学习基础">
      <i class="fa fa-chevron-left"></i> 机器学习基础
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习基础"><span class="nav-text">深度学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数学基础"><span class="nav-text">数学基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵的特征值和特征向量"><span class="nav-text">矩阵的特征值和特征向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵的秩"><span class="nav-text">矩阵的秩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵的奇异值"><span class="nav-text">矩阵的奇异值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵的低秩近似"><span class="nav-text">矩阵的低秩近似</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度学习基础-1"><span class="nav-text">深度学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#机器学习三要素"><span class="nav-text">机器学习三要素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#策略设计：训练误差-泛化误差"><span class="nav-text">策略设计：训练误差 -&gt; 泛化误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#策略设计：无免费午餐定理"><span class="nav-text">策略设计：无免费午餐定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#策略设计：奥卡姆剃刀原理"><span class="nav-text">策略设计：奥卡姆剃刀原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#频率学派-vs-贝叶斯学派"><span class="nav-text">频率学派 vs 贝叶斯学派</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#统计机器学习"><span class="nav-text">统计机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#相关性和因果性"><span class="nav-text">相关性和因果性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络"><span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的损失函数"><span class="nav-text">常用的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本组成结构"><span class="nav-text">基本组成结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一维卷积"><span class="nav-text">一维卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基本概念"><span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络经典结构"><span class="nav-text">卷积神经网络经典结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AlexNet"><span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG"><span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GoogleNet"><span class="nav-text">GoogleNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet"><span class="nav-text">ResNet</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Leo"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Leo</p>
  <div class="site-description" itemprop="description">I just want to do what I want.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/249606097" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;249606097" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Leo69041494" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Leo69041494" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-cn" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leo</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共17.1k字</span>
</div>
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
